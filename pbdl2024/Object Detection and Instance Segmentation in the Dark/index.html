<!DOCTYPE html>
<html lang="en">
<head>
<title>Challenge</title>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="Conference project">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../styles/bootstrap4/bootstrap.min.css">
<link href="../plugins/font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/owl.carousel.css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/owl.theme.default.css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/animate.css">
<link rel="stylesheet" type="text/css" href="../styles/speakers.css">
<link rel="stylesheet" type="text/css" href="../styles/speakers_responsive.css">
<link rel="stylesheet" type="text/css" href="../styles/new_challenge.css">

<script type="text/javascript" src="../js/jquery-3.4.1.min.js"></script>


</head>
<body>

<div class="super_container">

	<!-- Menu -->
    
	<div class="menu trans_500">
		<div class="menu_content d-flex flex-column align-items-center justify-content-center text-center">
			<div class="menu_close_container"><div class="menu_close"></div></div>
			<div class="hamburger ml-auto"><i class="fa fa-bars" aria-hidden="true"></i></div>
			<ul>
				<li class="menu_item"><a href="../preview_index.html">Home</a></li>
				<li class="menu_item"><a href="../organizers.html">Organizers</a></li>
				<li class="menu_item"><a href="../speakers.html">Speakers</a></li>
				<li class="menu_item"><a href="../callforpapers.html">Call for papers</a></li>
				<li class="menu_item"><a href="../challenge/index.html">Challenges</a></li>
				<li class="menu_item"><a href="../program.html">Program</a></li>
			</ul>
		</div>
	</div>
    
	<!-- Home -->

	<div class="home">
		<div class="parallax_background parallax-window" data-parallax="scroll" data-image-src="background.jpg" data-speed="0.8"></div>

		<!-- Header -->

		<header class="header" id="header">
				<div class="header_nav" id="header_nav_pin">
					<div class="header_nav_inner">
						<div class="header_nav_container">
							<div class="container">
								<div class="row">
									<div class="col">
										<div class="header_nav_content d-flex flex-row align-items-center justify-content-start">
											<nav class="main_nav">
												<ul>
													<li><a href="../preview_index.html">Home</a></li>
													<li><a href="../organizers.html">Organizers</a></li>
													<li><a href="../speakers.html">Speakers</a></li>
													<li><a href="../callforpapers.html">Call for papers</a></li>
													<li class="active"><a href="../challenge/index.html">Challenges</a></li>
													<li><a href="../program.html">Program</a></li>
													
												</ul>
											</nav>
											
										</div>
									</div>
								</div>
							</div>
						</div>
						
					</div>
				</div>	
			</div>
		</header>
		<div class="home_content_container">
			<div class="container">
				<div class="row">
					<div class="col">
						<div class="home_content">
							<div class="home_date">The challenge on</div>
							<div class="home_title">
								<font color="red">O</font>bject <font color="red">D</font>etection and <font
									color="red">I</font>nstance <font color="red">S</font>egmentation in the <font color="red">D</font>ark
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
		
	</div>

	<!-- Speakers -->

	 <div id="site_content">
      

	  <div><!--new body-->
        <div class="container">
				<div class="row">
					<div class="col-lg-2 col-md-4"><!--小导航部分-->

						

						<ul class="nav nav-pills flex-column new_challenge">
							<li class="nav-item"><a class="nav-link" href="important_dates.html">Important Dates</a></li>
							<li class="nav-item"><a class="nav-link active" href="index.html">Introduction</a></li>
							<li class="nav-item"><a class="nav-link" href="dataset_generation.html">Dataset</a></li>
							<li class="nav-item"><a class="nav-link" href="download.html">Download</a></li>
							<li class="nav-item"><a class="nav-link" href="evaluation.html">Evaluation</a></li>
							<!-- <li class="nav-item"><a class="nav-link" href="results.html">Results</a></li> -->
							<!-- <li class="nav-item"><a class="nav-link" href="acknowledgement_and_references.html">Acknowledgement and References</a></li> -->
						</ul>

					</div>
					<div class="col-lg-10 col-md-8"><!--内容部分-->

						<p style="font-size:16px;">
							<b style="font-family:Times New Roman;color:black;font-size:30px;">
								Low-light Object Detection and Instance Segmentation
							</b>
						</p>
						<div class="row" style="position: relative">
							<div class="col-lg-6 offset-lg-3 col-md-12">
								<img src="dataset.png" class="img-responsive" style="width: 150%; margin-left: -110px;">
							</div>
						</div>
                        
                        <div class="row" style="margin-bottom:1.5rem">
                            <div class="col-lg-12 col-md-12">
                                <p class="text-center font-italic" style="line-height:1.2rem; margin-top: 0.4rem "><b>Fig.1. </b>An example of the Object Detection and Instance Segmentation in the Dark Dataset,<br/>Four image types (long-exposure normal-light and short-exposure low-light images in both RAW and sRGB formats) are captured for each scene</p>
                            </div>
                        </div>
                        
                        <!-- <div class="row" style="position: relative">
                            <div class="col-lg-6 offset-lg-3 col-md-12">
                                <img src="../images/Fig.2.png" class="img-responsive" style="width: 100%;margin-top: 0.4rem">
                            </div>
                        </div>
                        
                        <div class="row" style="margin-bottom: 1.4rem">
                            <div class="col-lg-12 col-md-12">
                                <p class="text-center font-italic" style="line-height:1.2rem; margin-top: 0.4rem "><b>Fig.2. </b>Outdoor data capturing in Shanghai. The camera system is top-mounted.</p>
                            </div>
                        </div> -->
                    
                    
                        <!-- <p style="font-size:16px;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Physics Based Vision Meets Deep Learning
							</b>
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							Light traveling in the 3D world interacts with the scene through intricate processes before being captured by a camera. These processes result in the dazzling effects like color and shading, complex surface and material appearance, different weathering, just to name a few. Physics based vision aims to invert the processes to recover the scene properties, such as shape, reflectance, light distribution, medium properties, etc., from the images by modeling and analysing the imaging process to extract desired features or information.
						</p>
						
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							There are many popular topics in physics based vision. Some examples are shape from shading, photometric stereo, reflectance modelling, reflection separation, radiometric calibration, intrinsic image decomposition, and so on. As a series of classic and fundamental problems in computer vision, physics based vision facilitates high-level computer vision problems from various aspects. For example, the estimated surface normal is a useful cue for 3D scene understanding; the specular-free image could significantly increase the accuracy of image recognition problem; the intrinsic images reflecting inherent properties of the objects in the scene substantially benefit other computer vision algorithms, such as segmentation, recognition; reflectance analysis serves as the fundamental support for material classification; and, bad weather visibility enhancement is important for outdoor vision systems.
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							In recent years, deep neural networks and learning techniques show promising improvement for various high-level vision tasks, such as detection, classification, tracking, etc. With the physics imaging formation model involved, successful examples can also be found in various physics based vision problems (please refer to the references section).
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							When physics based vision meets deep learning, there will be mutual benefits. On one hand, classic physics based vision tasks can be implemented in a data-fashion way to handle complex scenes. This is because, a physically more accurate optical model can be too complex as an inverse problem for computer vision algorithms (usually too many unknown parameters in one model), however, it can be well approximated providing a sufficient collection of data. Later, the intrinsic physical properties are likely to be learned through a deep neural network model. Existing research has already exploited such benefit on luminance transfer, computational stereo, haze removal, etc.
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							On the other hand, high-level vision task can also be benefited by awareness of the physics principles. For instance, physics principles can be utilized to supervise the learning process, by explicitly extracting the low-level physical principles rather than learning it implicitly. In this way, the network could be more accurate more efficient. Such physics principles have already presented the benefits in semantic segmentation, object detection, etc. Therefore, we believe when physics based vision meets deep learning both low level and high level vision task can get the benefits. Furthermore, we believe that there are many computer vision tasks that can be tackled by solving both physics based vision and high level vision in a joint fashion to get more robust and accurate results which cannot be achieved by ignoring each side.
						</p>
                           -->
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Low-light Object Detection and Instance Segmentation
							</b>
						</p>
						
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							In comparison to well-lit environments, low-light conditions pose significant challenges to maintaining image quality, often resulting in notable degradation such as loss of detail, color distortion, and pronounced noise. These factors detrimentally impact the performance of downstream visual tasks, particularly object detection and instance segmentation. Recognizing the critical importance of overcoming these obstacles, research into low-light object detection and instance segmentation has emerged as a pivotal area within the computer vision community, aiming to accurately localize and classify objects of interest under challenging lighting conditions. 
						</p>

						<p style="font-size:16px;font-variant-numeric: lining-nums;">
							To propel research in this field forward, it is essential to assess proposed methods in real-world scenarios, where lighting conditions and image noise are inherently more complex and diverse. Consequently, we will utilize the Low-light Instance Segmentation (LIS) dataset, introduced by Prof. Fu’s team in [a], captured using a Canon EOS 5D Mark IV camera. The LIS dataset comprises paired images collected across various scenes, encompassing both indoor and outdoor environments. To ensure a comprehensive range of low-light conditions, we utilized different ISO levels (e.g., 800, 1600, 3200, 6400) for long-exposure reference images and deliberately adjusted exposure times using varying low-light factors (e.g., 10, 20, 30, 40, 50, 100) to simulate extremely low-light conditions accurately. Each image pair in the LIS dataset includes instances of common object classes (bicycle, car, motorcycle, bus, bottle, chair, dining table, TV), accompanied by precise instance-level pixel-wise labels. These annotations serve as essential metrics for evaluating the performance of proposed methods in terms of object detection and instance segmentation. We will host the competition using open source online platform, e.g. CodaLab. All submissions are evaluated by our script running on the server and we will double check the results of top-rank methods manually before releasing the final test-set rating.
						</p>

                        <p style="font-size:16px; margin-bottom: 20px">
							<b style="font-family:arial;color:black;font-size:16px;">
								<a href="https://link.springer.com/article/10.1007/s11263-023-01808-8">
									[a] Chen, L., Fu, Y., Wei, K., Zheng, D., & Heide, F. (2023). Instance Segmentation in the Dark. International Journal of Computer Vision
								</a>
							</b>
						</p>

					</div>
				</div>
			</div>
    </div><!--new body end-->
    </div>
</div>

<!--<script src="../static/js/jquery-3.2.1.min.js"></script>-->
<script src="../styles/bootstrap4/popper.js"></script>
<script src="../styles/bootstrap4/bootstrap.min.js"></script>
<script src="../plugins/OwlCarousel2-2.2.1/owl.carousel.js"></script>
<script src="../plugins/easing/easing.js"></script>
<script src="../plugins/parallax-js-master/parallax.min.js"></script>
<script src="../js/speakers.js"></script>
</body>
</html>