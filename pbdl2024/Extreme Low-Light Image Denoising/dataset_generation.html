<!DOCTYPE html>
<html lang="en">
<head>
<title>Challenge</title>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="Conference project">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../styles/bootstrap4/bootstrap.min.css">
<link href="../plugins/font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/owl.carousel.css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/owl.theme.default.css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/animate.css">
<link rel="stylesheet" type="text/css" href="../styles/speakers.css">
<link rel="stylesheet" type="text/css" href="../styles/speakers_responsive.css">
<link rel="stylesheet" type="text/css" href="../styles/new_challenge.css">

<script type="text/javascript" src="../js/jquery-3.4.1.min.js"></script>


</head>
<body>

<div class="super_container">

	<!-- Menu -->
    
	<div class="menu trans_500">
		<div class="menu_content d-flex flex-column align-items-center justify-content-center text-center">
			<div class="menu_close_container"><div class="menu_close"></div></div>
			<div class="hamburger ml-auto"><i class="fa fa-bars" aria-hidden="true"></i></div>
			<ul>
				<li class="menu_item"><a href="../index.html">Home</a></li>
				<li class="menu_item"><a href="../organizers.html">Organizers</a></li>
				<li class="menu_item"><a href="../speakers.html">Speakers</a></li>
				<li class="menu_item"><a href="../callforpapers.html">Call for papers</a></li>
				<li class="menu_item"><a href="../challenge/index.html">Challenges</a></li>
				<li class="menu_item"><a href="../program.html">Program</a></li>
			</ul>
		</div>
	</div>
    
	<!-- Home -->

	<div class="home">
		<div class="parallax_background parallax-window" data-parallax="scroll" data-image-src="background.jpg" data-speed="0.8"></div>

		<!-- Header -->

		<header class="header" id="header">
				<div class="header_nav" id="header_nav_pin">
					<div class="header_nav_inner">
						<div class="header_nav_container">
							<div class="container">
								<div class="row">
									<div class="col">
										<div class="header_nav_content d-flex flex-row align-items-center justify-content-start">
											<nav class="main_nav">
												<ul>
													<li><a href="../index.html">Home</a></li>
													<li><a href="../organizers.html">Organizers</a></li>
													<li><a href="../speakers.html">Speakers</a></li>
													<li><a href="../callforpapers.html">Call for papers</a></li>
													<li class="active"><a href="../challenge/index.html">Challenges</a></li>
													<li><a href="../program.html">Program</a></li>
													
												</ul>
											</nav>
											
										</div>
									</div>
								</div>
							</div>
						</div>
						
					</div>
				</div>	
			</div>
		</header>
		<div class="home_content_container">
			<div class="container">
				<div class="row">
					<div class="col">
						<div class="home_content">
							<div class="home_date">The challenge on</div>
							<div class="home_title">
								<font color="red">E</font>xtreme<font color="red">L</font>ow-light <font color="red">I</font>mage </div>
							<div class="home_title">	
								<font color="red">D</font>enoising
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
		
	</div>

	<!-- Speakers -->

	 <div id="site_content">
      

	  <div><!--new body-->
        <div class="container">
				<div class="row">
					<div class="col-lg-2 col-md-4"><!--小导航部分-->

						

						<ul class="nav nav-pills flex-column new_challenge">
							<li class="nav-item"><a class="nav-link" href="important_dates.html">Important Dates</a></li>
							<li class="nav-item"><a class="nav-link" href="index.html">Introduction</a></li>
							<li class="nav-item"><a class="nav-link active" href="dataset_generation.html">Dataset</a></li>
							<li class="nav-item"><a class="nav-link" href="download.html">Download</a></li>
							<li class="nav-item"><a class="nav-link" href="evaluation.html">Evaluation</a></li>
							<!-- <li class="nav-item"><a class="nav-link" href="results.html">Results</a></li> -->
							<!-- <li class="nav-item"><a class="nav-link" href="acknowledgement_and_references.html">Acknowledgement and References</a></li> -->
						</ul>

					</div>
					<div class="col-lg-10 col-md-8"><!--内容部分-->

						<p style="font-size:20px;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Data Collection
							</b>
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<!-- <b class="font-italic">
								Motivation 
							</b>
							<br/> -->
							To systematically study the generality of the proposed noise formation model, we collect an extreme low-light dataset that covers 10 indoor scenes and 4 camera devices from multiple brands (i.e., SonyA7S2, NikonD850, CanonEOS70D, CanonEOS700D) for benchmarking. We also record bias and flat-field frames for each camera to calibrate our noise model. The data capture setup is shown in Figure 1(a). The camera is mounted on a sturdy optical table and controlled by a remote software to avoid misalignments caused by camera motion, and the scene is illuminated by natural or direct current light sources to avoid flickering effect of alternating current lights. For each scene of a given camera, a reference image at the base ISO was firstly taken, followed by noisy images whose exposure time was deliberately decreased by low light factors f to simulate extreme low light conditions. Another reference image then was taken akin to the first one, to ensure no accidental errors (e.g., drastic illumination change or accidental camera/scene motion) occurred. The detailed procedures are presented in Algorithm 1. We choose three ISO levels (800, 1600, and 3200)5 and two low light factors (100, 200) for noisy images to capture our dataset, resulting in 240 (3×2×10×4) raw image pairs in total. The hardest example in our dataset resembles the image captured at a “pseudo” ISO up to 640000(3200×200).
						</p>

                        <!-- <p style="font-size:16px;">
							<b class="font-italic">
								Outdoor data collection in Shanghai 
							</b>
							<br/>
							The dataset collection is in Shanghai for three days in June. The we collected data in a variety of environment including: crowded traffic area, famous buildings and structures, CBD, highways, quiet suburbs, overpasses and underground parking. The weather condition includes sunny and cloudy days. And the lighting condition includes day, night and sunset. We use standard color board for color calibration. We collecting data, the car is driving at a speed in the range of 20-50km/h. The hyperspectral camera is working at 1fps. The field of view (FOV) is 16 × 12 degrees in current lens configuration. The camera is vertical mounted to enable capturing of a wider dynamic range.
						</p> -->

                        <div class="row">
                            <div class="col-lg-6 offset-lg-3 col-md-12">
                                <img src="dataset_1.png" class="img-responsive" style="width: 200%;margin-left: -250px;">
                            </div>
                        </div>
                        <!-- <div class="row" style="margin-bottom:1.5rem">
                            <div class="col-lg-12 col-md-12">
                                <p class="text-center font-italic" style="line-height:1.2rem; margin-top: 0.4rem ">
									<b>
										Fig.3. 
									</b>
									Specifications of the hyperspectral data
								</p>
                            </div>
                        </div> -->

                        <!-- <p style="font-size:20px;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Dataset characteristics
							</b>
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b class="font-italic">
								2000 pairs
							</b>
							<br/>
							It contains 2,000 image pairs, which is four times the size of the LOL dataset.
						</p>
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b class="font-italic">
								No repeated scenes 
							</b>
							<br/>
							Different from the existing real scenes dataset, i.e., LOL, there are no repeated scenes in our PNLI dataset, which is more abundant and superior than LOL.
						</p> -->
                        <!-- <p style="font-size:16px;">
							We use Computer Vision Annotation Tool (CVAT) as the labeling tool. And the labels and colors are consistent with CityScape[1] dataset.
						</p> -->
                        <!-- <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b class="font-italic">
								Object categoriesare rich and common 
							</b>
							<br/>
							All images in PNLI are collected from considerably more real scenes, which contain both indoor and outdoor scenes. In addition, the object categories in images are rich and common.
						</p>
						<p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b class="font-italic">
								Excellent visual quality and clarity 
							</b>
							<br/>
							Excellent visual quality and clarity, which might help in learning pixel-level contextual information.
						</p>
						<p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b class="font-italic">
								Rich darkness levels 
							</b>
							<br/>
							The darkness levels of low-light images in PNLI are rich, and it can truly restore various situations where the actual image brightness is missing due to insufficient ambient light or human operation mistakes. Therefore, it can effectively verify the stability and robustness of our proposed method.
						</p> -->
                        <!-- <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Hyperspectral Image Compression
							</b>
						</p>
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							Unlike RGB images which only has 3 channels, each hyperspectral images in our dataset has 128 channels. Without compression, each hyperspectral cubic will take more than 1Gigabytes storage and will make it unreasonable for online transfer. In our current release, we compress the image using H.264 encoder and decoder with a quality setting at 90%. The compression ratio is smaller than 2% and the final dataset size is around 150 Gigabytes.
						</p> -->


					</div>
				</div>
			</div>
    </div><!--new body end-->
    </div>
</div>

<!--<script src="../static/js/jquery-3.2.1.min.js"></script>-->
<script src="../styles/bootstrap4/popper.js"></script>
<script src="../styles/bootstrap4/bootstrap.min.js"></script>
<script src="../plugins/OwlCarousel2-2.2.1/owl.carousel.js"></script>
<script src="../plugins/easing/easing.js"></script>
<script src="../plugins/parallax-js-master/parallax.min.js"></script>
<script src="../js/speakers.js"></script>
</body>
</html>