<!DOCTYPE html>
<html lang="en">
<head>
<title>Challenge</title>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="description" content="Conference project">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../styles/bootstrap4/bootstrap.min.css">
<link href="../plugins/font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/owl.carousel.css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/owl.theme.default.css">
<link rel="stylesheet" type="text/css" href="../plugins/OwlCarousel2-2.2.1/animate.css">
<link rel="stylesheet" type="text/css" href="../styles/speakers.css">
<link rel="stylesheet" type="text/css" href="../styles/speakers_responsive.css">
<link rel="stylesheet" type="text/css" href="../styles/new_challenge.css">

<script type="text/javascript" src="../js/jquery-3.4.1.min.js"></script>


</head>
<body>

<div class="super_container">

	<!-- Menu -->
    
	<div class="menu trans_500">
		<div class="menu_content d-flex flex-column align-items-center justify-content-center text-center">
			<div class="menu_close_container"><div class="menu_close"></div></div>
			<div class="hamburger ml-auto"><i class="fa fa-bars" aria-hidden="true"></i></div>
			<ul>
				<li class="menu_item"><a href="../index.html">Home</a></li>
				<li class="menu_item"><a href="../organizers.html">Organizers</a></li>
				<li class="menu_item"><a href="../speakers.html">Speakers</a></li>
				<li class="menu_item"><a href="../callforpapers.html">Call for papers</a></li>
				<li class="menu_item"><a href="../challenge/index.html">Challenges</a></li>
				<li class="menu_item"><a href="../program.html">Program</a></li>
			</ul>
		</div>
	</div>
    
	<!-- Home -->

	<div class="home">
		<div class="parallax_background parallax-window" data-parallax="scroll" data-image-src="background.jpg" data-speed="0.8"></div>

		<!-- Header -->

		<header class="header" id="header">
				<div class="header_nav" id="header_nav_pin">
					<div class="header_nav_inner">
						<div class="header_nav_container">
							<div class="container">
								<div class="row">
									<div class="col">
										<div class="header_nav_content d-flex flex-row align-items-center justify-content-start">
											<nav class="main_nav">
												<ul>
													<li><a href="../index.html">Home</a></li>
													<li><a href="../organizers.html">Organizers</a></li>
													<li><a href="../speakers.html">Speakers</a></li>
													<li><a href="../callforpapers.html">Call for papers</a></li>
													<li class="active"><a href="../challenge/index.html">Challenges</a></li>
													<li><a href="../program.html">Program</a></li>
													
												</ul>
											</nav>
											
										</div>
									</div>
								</div>
							</div>
						</div>
						
					</div>
				</div>	
			</div>
		</header>
		<div class="home_content_container">
			<div class="container">
				<div class="row">
					<div class="col">
						<div class="home_content">
							<div class="home_date">The challenge on</div>
							<div class="home_title">
								<font color="red">H</font>ighspeed <font color="red">H</font>DR <font color="red">V</font>ideo <font color="red">R</font>econstruction</div>
							<div class="home_title">	
								<font color="red">F</font>rom <font color="red">E</font>vent 
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
		
	</div>

	<!-- Speakers -->

	 <div id="site_content">
      

	  <div><!--new body-->
        <div class="container">
				<div class="row">
					<div class="col-lg-2 col-md-4"><!--小导航部分-->

						

						<ul class="nav nav-pills flex-column new_challenge">
							<li class="nav-item"><a class="nav-link" href="important_dates.html">Important Dates</a></li>
							<li class="nav-item"><a class="nav-link active" href="index.html">Introduction</a></li>
							<li class="nav-item"><a class="nav-link" href="dataset_generation.html">Dataset</a></li>
							<li class="nav-item"><a class="nav-link" href="download.html">Download</a></li>
							<li class="nav-item"><a class="nav-link" href="evaluation.html">Evaluation</a></li>
							<!-- <li class="nav-item"><a class="nav-link" href="results.html">Results</a></li> -->
							<!-- <li class="nav-item"><a class="nav-link" href="acknowledgement_and_references.html">Acknowledgement and References</a></li> -->
						</ul>

					</div>
					<div class="col-lg-10 col-md-8"><!--内容部分-->

						<p style="font-size:16px;">
							<b style="font-family:Times New Roman;color:black;font-size:30px;">
								Highspeed HDR Video Reconstruction from Events
							</b>
						</p>
						<div class="row" style="position: relative">
							<div class="col-lg-6 offset-lg-3 col-md-12">
								<img src="dataset.png" class="img-responsive" style="width: 150%; margin-left: -110px;">
							</div>
						</div>
                        
                        <div class="row" style="margin-bottom:1.5rem">
                            <div class="col-lg-12 col-md-12">
                                <p class="text-center font-italic" style="line-height:1.2rem; margin-top: 0.4rem "><b>Fig.1. </b>Representative examples for Event-to-HDR dataset.. For each scene, a event stream captured by real event cameras are used as input data, and the HDR video frame that is merged by bracket exposures are used as ground truth.</p>
                            </div>
                        </div>
                        
                        <!-- <div class="row" style="position: relative">
                            <div class="col-lg-6 offset-lg-3 col-md-12">
                                <img src="../images/Fig.2.png" class="img-responsive" style="width: 100%;margin-top: 0.4rem">
                            </div>
                        </div>
                        
                        <div class="row" style="margin-bottom: 1.4rem">
                            <div class="col-lg-12 col-md-12">
                                <p class="text-center font-italic" style="line-height:1.2rem; margin-top: 0.4rem "><b>Fig.2. </b>Outdoor data capturing in Shanghai. The camera system is top-mounted.</p>
                            </div>
                        </div> -->
                    
                    
                        <!-- <p style="font-size:16px;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Physics Based Vision Meets Deep Learning
							</b>
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							Light traveling in the 3D world interacts with the scene through intricate processes before being captured by a camera. These processes result in the dazzling effects like color and shading, complex surface and material appearance, different weathering, just to name a few. Physics based vision aims to invert the processes to recover the scene properties, such as shape, reflectance, light distribution, medium properties, etc., from the images by modeling and analysing the imaging process to extract desired features or information.
						</p>
						
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							There are many popular topics in physics based vision. Some examples are shape from shading, photometric stereo, reflectance modelling, reflection separation, radiometric calibration, intrinsic image decomposition, and so on. As a series of classic and fundamental problems in computer vision, physics based vision facilitates high-level computer vision problems from various aspects. For example, the estimated surface normal is a useful cue for 3D scene understanding; the specular-free image could significantly increase the accuracy of image recognition problem; the intrinsic images reflecting inherent properties of the objects in the scene substantially benefit other computer vision algorithms, such as segmentation, recognition; reflectance analysis serves as the fundamental support for material classification; and, bad weather visibility enhancement is important for outdoor vision systems.
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							In recent years, deep neural networks and learning techniques show promising improvement for various high-level vision tasks, such as detection, classification, tracking, etc. With the physics imaging formation model involved, successful examples can also be found in various physics based vision problems (please refer to the references section).
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							When physics based vision meets deep learning, there will be mutual benefits. On one hand, classic physics based vision tasks can be implemented in a data-fashion way to handle complex scenes. This is because, a physically more accurate optical model can be too complex as an inverse problem for computer vision algorithms (usually too many unknown parameters in one model), however, it can be well approximated providing a sufficient collection of data. Later, the intrinsic physical properties are likely to be learned through a deep neural network model. Existing research has already exploited such benefit on luminance transfer, computational stereo, haze removal, etc.
						</p>

                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							On the other hand, high-level vision task can also be benefited by awareness of the physics principles. For instance, physics principles can be utilized to supervise the learning process, by explicitly extracting the low-level physical principles rather than learning it implicitly. In this way, the network could be more accurate more efficient. Such physics principles have already presented the benefits in semantic segmentation, object detection, etc. Therefore, we believe when physics based vision meets deep learning both low level and high level vision task can get the benefits. Furthermore, we believe that there are many computer vision tasks that can be tackled by solving both physics based vision and high level vision in a joint fashion to get more robust and accurate results which cannot be achieved by ignoring each side.
						</p>
                           -->
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							<b style="font-family:arial;color:black;font-size:16px;">
								Highspeed HDR Video Reconstruction from Events
							</b>
						</p>
						<p style="font-size:16px;font-variant-numeric: lining-nums;">
							Event cameras, differing from conventional cameras that capture scene intensities at a fixed frame rate, use a unique approach by detecting pixel-wise intensity changes asynchronously. This is triggered whenever a pixel's intensity change surpasses a certain contrast threshold. Unlike traditional frame-based cameras, event cameras have several advantages: low latency, low power consumption, high temporal resolution, and high dynamic range (HDR). These qualities make them particularly useful for a range of vision tasks, including real-time object tracking, high-speed motion estimation, face recognition, optical flow estimation, depth map prediction, ego motion analysis, and onboard robotics applications.
						</p>
						
                        <p style="font-size:16px;font-variant-numeric: lining-nums;">
							However, the distinct triggering mechanism of event cameras presents a challenge. The event data they capture, which lacks absolute intensity values and is represented as 4-tuples, is incompatible with standard frame-based vision algorithms. This discrepancy necessitates specialized processing pipelines, different from traditional image processing methods. Consequently, there is a growing interest in transforming event data into intensity images to leverage the high-speed and HDR capabilities of event cameras in practical applications.
						</p>

						<p style="font-size:16px;font-variant-numeric: lining-nums;">
							To this end, we are launching a challenge focused on reconstructing high-speed HDR videos from event streams. We will utilize the high-quality Event-to-HDR dataset, captured by a co-axis system and developed by Prof. Fu's team as noted in [a]. This dataset includes aligned pairs of event streams and HDR videos in both spatial and temporal dimensions. The four-tuple event streams will serve as the input, while the ground truth will be HDR images merged from two high-speed cameras. The dataset features a high frame rate of 500fps. We will host the competition using open source online platform, e.g. CodaLab. All submissions are evaluated by our script running on the server and we will double check the results of top-rank methods manually before releasing the final test-set rating.
						</p>

                        <p style="font-size:16px; margin-bottom: 20px">
							<b style="font-family:arial;color:black;font-size:16px;">
								<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zou_Learning_To_Reconstruct_High_Speed_and_High_Dynamic_Range_Videos_CVPR_2021_paper.html">
									[a] Zou, Yunhao, et al. "Learning to reconstruct high speed and high dynamic range videos from events." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021
								</a>
							</b>
						</p>

					</div>
				</div>
			</div>
    </div><!--new body end-->
    </div>
</div>

<!--<script src="../static/js/jquery-3.2.1.min.js"></script>-->
<script src="../styles/bootstrap4/popper.js"></script>
<script src="../styles/bootstrap4/bootstrap.min.js"></script>
<script src="../plugins/OwlCarousel2-2.2.1/owl.carousel.js"></script>
<script src="../plugins/easing/easing.js"></script>
<script src="../plugins/parallax-js-master/parallax.min.js"></script>
<script src="../js/speakers.js"></script>
</body>
</html>